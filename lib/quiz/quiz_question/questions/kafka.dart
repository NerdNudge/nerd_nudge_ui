import 'dart:convert';

class ApacheKafkaQuestions {
  int index = 0;

  List questions = [
    json.decode('{"title":"Primary Use of Apache Kafka","question":"What is Apache Kafka primarily used for?","possible_answers":{"A":"Data integration","B":"Real-time data streaming","C":"Batch data processing","D":"Machine learning"},"answer_percentages":{"A":10.0,"B":80.0,"C":5.0,"D":5.0},"right_answer":"B","topic_name":"Basics of Kafka","description_and_explanation":"Apache Kafka is a distributed event streaming platform capable of handling trillions of events a day. It is primarily used for building real-time streaming data pipelines and applications that adapt to data streams.","difficulty_level":"Easy","pro_tip":"Utilize Kafka for scenarios that require real-time analytics and event-driven architecture.","fun_fact":"Kafka was originally developed by LinkedIn in 2011 to handle its growing data and activity stream."}'),
    json.decode('{"title":"Kafka Broker Functionality","question":"Which component in Kafka is responsible for storing and serving data records?","possible_answers":{"A":"Producer","B":"Consumer","C":"Broker","D":"Connector"},"answer_percentages":{"A":10.0,"B":10.0,"C":70.0,"D":10.0},"right_answer":"C","topic_name":"Kafka Architecture","description_and_explanation":"In Apache Kafka, a broker is a server that is part of the Kafka cluster. Brokers are responsible for maintaining published data. Each broker may have zero or more partitions per topic, and these partitions are the ones that actually store the data.","difficulty_level":"Medium","pro_tip":"Ensure that your Kafka cluster has multiple brokers to guarantee data redundancy and high availability.","fun_fact":"A single Kafka broker can handle hundreds of megabytes of reads and writes per second from thousands of clients."}'),
    json.decode('{"title":"Kafka Replication Mechanics","question":"What feature allows Kafka to automatically balance partitions across brokers?","possible_answers":{"A":"Partitioning","B":"Replication","C":"Log compaction","D":"Topic discovery"},"answer_percentages":{"A":5.0,"B":10.0,"C":5.0,"D":80.0},"right_answer":"B","topic_name":"Kafka Replication","description_and_explanation":"Kafka\'s replication feature ensures that partitions are replicated across a configurable number of brokers. This not only helps in balancing the load across the brokers but also provides fault tolerance in case of a broker failure.","difficulty_level":"Medium","pro_tip":"Use replication factors wisely to balance between performance and data safety.","fun_fact":"Replication is a key factor in achieving high availability and durability in Kafka."}'),
    json.decode('{"title":"Consuming Data with Kafka","question":"Which Kafka API is used to consume data from a Kafka cluster?","possible_answers":{"A":"Producer API","B":"Consumer API","C":"Streams API","D":"Connector API"},"answer_percentages":{"A":10.0,"B":70.0,"C":10.0,"D":10.0},"right_answer":"B","topic_name":"Kafka Consumer API","description_and_explanation":"The Kafka Consumer API allows applications to read streams of data from topics in the Kafka cluster. This API handles complexities of partition rebalancing, fault tolerance, and interacting with the Kafka cluster.","difficulty_level":"Easy","pro_tip":"Utilize the Consumer API to build robust systems that can handle data pipelines reliably.","fun_fact":"The Consumer API can subscribe to multiple topics, providing great flexibility in data processing architectures."}'),
    json.decode('{"title":"Real-Time Data Processing with Kafka Streams","question":"What is the purpose of Kafka Streams?","possible_answers":{"A":"To connect Kafka with external systems","B":"To manage Kafka clusters","C":"To process data in real-time within Kafka","D":"To replicate data across multiple data centers"},"answer_percentages":{"A":5.0,"B":5.0,"C":85.0,"D":5.0},"right_answer":"C","topic_name":"Kafka Streams","description_and_explanation":"Kafka Streams is a client library for building applications and microservices where the input and output data are stored in Kafka clusters. It enables real-time data processing and analytics within the Kafka ecosystem.","difficulty_level":"Medium","pro_tip":"Leverage Kafka Streams for complex event processing and real-time analytics directly on the data residing in Kafka.","fun_fact":"Kafka Streams was introduced in Kafka 0.10 and does not require a separate processing cluster."}'),
    json.decode('{"title":"Understanding Kafka Topics","question":"What is a Kafka Topic?","possible_answers":{"A":"A queue for storing messages","B":"A log of replicated data","C":"A category or feed name to which records are published","D":"A storage layer within Kafka"},"answer_percentages":{"A":5.0,"B":10.0,"C":80.0,"D":5.0},"right_answer":"C","topic_name":"Kafka Basics","description_and_explanation":"In Kafka, a topic is a category or feed name to which records are published. Topics in Kafka are always multi-subscriber; that is, a topic can have zero, one, or many consumers that subscribe to the data written to it.","difficulty_level":"Easy","pro_tip":"Design your topic namespace wisely to ensure that data is organized logically and efficiently for consumers.","fun_fact":"Kafka topics are designed to provide a publishing/subscribing mechanism that is fault-tolerant and scalable."}'),
    json.decode('{"title":"Ensuring Message Durability in Kafka","question":"How does Kafka ensure message durability?","possible_answers":{"A":"By storing messages in memory","B":"By writing messages to disk","C":"By replicating messages across the network","D":"By encrypting messages"},"answer_percentages":{"A":5.0,"B":70.0,"C":20.0,"D":5.0},"right_answer":"B","topic_name":"Durability in Kafka","description_and_explanation":"Kafka ensures message durability by writing all messages to disk. This method guarantees that data is not lost even if the server crashes, as the data can be recovered from disk.","difficulty_level":"Medium","pro_tip":"Always monitor disk I/O and storage to maintain high performance and durability of your Kafka system.","fun_fact":"Despite writing to disk, Kafka achieves high throughput and low latency by optimizing disk I/O."}'),
    json.decode('{"title":"Leadership in Kafka Partitions","question":"What does it mean when a Kafka partition is said to be leader for a given partition?","possible_answers":{"A":"It handles all read and write requests for the partition","B":"It coordinates the replication of data","C":"It manages consumer connections","D":"It automatically balances partitions"},"answer_percentages":{"A":80.0,"B":10.0,"C":5.0,"D":5.0},"right_answer":"A","topic_name":"Kafka Partitioning","description_and_explanation":"In Kafka, each partition has one server which acts as the leader. The leader handles all read and write requests for the partition while other servers simply replicate the data. This setup ensures consistency and high availability of data.","difficulty_level":"Hard","pro_tip":"Ensure that leaders are distributed across different racks or servers to prevent data loss in case of hardware failure.","fun_fact":"The leader-partition architecture is a key design element that enhances Kafka\'s scalability and fault-tolerance."}'),
    json.decode('{"title":"Tracking Consumption with Kafka Offsets","question":"Which Kafka component is responsible for tracking the state of what has been consumed?","possible_answers":{"A":"Kafka Manager","B":"Zookeeper","C":"Consumer Group","D":"Offset"},"answer_percentages":{"A":5.0,"B":5.0,"C":5.0,"D":85.0},"right_answer":"D","topic_name":"Kafka Offset","description_and_explanation":"In Kafka, the offset is a unique identifier of records within a partition that tracks the state of what has been consumed. Each consumer group tracks its own offset per topic, which allows consumers to know where they are in the stream.","difficulty_level":"Medium","pro_tip":"Manage offsets carefully to ensure that your consumer groups are processing data correctly.","fun_fact":"Offsets are a critical feature for Kafka\'s ability to replay or rewind data streams as needed."}'),
    json.decode('{"title":"Load Balancing in Kafka","question":"What mechanism does Kafka use to balance load between the producers and the brokers?","possible_answers":{"A":"Round-robin partitioning","B":"Custom partitioner","C":"Load balancer","D":"Replication controller"},"answer_percentages":{"A":40.0,"B":50.0,"C":5.0,"D":5.0},"right_answer":"B","topic_name":"Kafka Load Balancing","description_and_explanation":"Kafka allows producers to use custom partitioners to distribute messages across topic partitions. This helps in balancing the load across brokers, especially when dealing with large volumes of data.","difficulty_level":"Hard","pro_tip":"Implement custom partitioners based on your data characteristics to optimize distribution and performance.","fun_fact":"The default partitioner in Kafka uses a hash of the key to assign a partition, ensuring even distribution under most conditions."}'),
    json.decode('{"title":"Simplifying Integration with Kafka Connect","question":"What is the main benefit of using Kafka Connect?","possible_answers":{"A":"To increase the speed of data processing","B":"To simplify the integration of Kafka with external systems","C":"To manage Kafka brokers","D":"To monitor Kafka performance"},"answer_percentages":{"A":10.0,"B":75.0,"C":5.0,"D":10.0},"right_answer":"B","topic_name":"Kafka Connect","description_and_explanation":"Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other data systems. Its primary benefit is simplifying the integration of Kafka with external systems such as databases, key-value stores, search indexes, and file systems, using configurable connectors.","difficulty_level":"Medium","pro_tip":"Use Kafka Connect to create reusable producers and consumers that connect Kafka topics to existing applications.","fun_fact":"Kafka Connect supports both source and sink connectors, enabling bidirectional data flow."}'),
    json.decode('{"title":"Disaster Recovery with Kafka MirrorMaker","question":"How does Kafka MirrorMaker help in disaster recovery?","possible_answers":{"A":"By duplicating data across multiple data centers","B":"By providing real-time backups","C":"By encrypting data","D":"By reducing data latency"},"answer_percentages":{"A":85.0,"B":10.0,"C":0.0,"D":5.0},"right_answer":"A","topic_name":"Kafka MirrorMaker","description_and_explanation":"Kafka MirrorMaker is a tool used to replicate data across multiple Kafka clusters, typically used for disaster recovery. By duplicating data across different geographic regions, it ensures that data is available even if one cluster goes down.","difficulty_level":"Medium","pro_tip":"Deploy MirrorMaker to replicate data across geographically dispersed data centers to enhance your disaster recovery strategies.","fun_fact":"MirrorMaker plays a crucial role in creating high availability Kafka environments, protecting against regional failures."}')
  ];

  ApacheKafkaQuestions._privateConstructor();
  static final ApacheKafkaQuestions _instance = ApacheKafkaQuestions._privateConstructor();

  factory ApacheKafkaQuestions() {
    return _instance;
  }

  getNextQuestion() {
    index = (index == questions.length - 1) ? 0 : index + 1;
    return questions[index];
  }

  getNextQuestionsList(int count) {
    index = (index == questions.length - 1) ? 0 : index + 1;
    var questionsList = [];
    for(int i = index; i < count; i ++) {
      questionsList.add(questions[index]);
      index = (index == questions.length - 1) ? 0 : index + 1;
    }
    return questionsList;
  }
}